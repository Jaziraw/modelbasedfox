---
title: "Marginal effects and derivatives"
output: 
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, estimate, estimate slopes, marginal effects]
vignette: >
  %\VignetteIndexEntry{Estimate marginal effects and derivatives}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = "")
knitr::opts_chunk$set(comment = ">", dpi = 450)
options(digits = 2)

if (!requireNamespace("ggplot2", quietly = TRUE) ||
  !requireNamespace("see", quietly = TRUE) ||
  !requireNamespace("gganimate", quietly = TRUE) ||
  !requireNamespace("rstanarm", quietly = TRUE) ||
  !requireNamespace("dplyr", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
}

set.seed(333)
```

This vignette will present how to estimate marginal effects and derivatives using
`estimate_slopes()`.

**Marginal means *vs.* Marginal effects**. Marginal slopes are to numeric predictors what marginal means are to categorical predictors, in the sense that they can eventually be "averaged over" other predictors of the model. The key difference is that, while marginal means return averages of the outcome variable (i.e., the y-axis is the outcome variable, which allows you to say for instance "the average reaction time in the C1 condition is 1366 ms"), marginal effects return averages of coefficients (i.e., the y-axis is the effect/slope/beta of a given numeric predictor).


# Marginal effects over a factor's levels

Let's fit a linear model with a factor interacting with a continuous predictor and visualize it. 

```{r message=FALSE, warning=FALSE}
library(parameters)
library(performance)
library(modelbased)

model <- lm(Sepal.Length ~ Petal.Length * Species, data = iris)

plot(estimate_relation(model))
```

It seems like the slope of the effect is roughly similar (in the same direction) across the different factor levels.

```{r message=FALSE, warning=FALSE}
parameters(model)
```

Moreover, the interaction is not significant. However, we see below that removing the interaction does not *substantially* improve the model's performance. So, for the sake of the demonstration, let's say we want to keep the maximal effect structure.

```{r message=FALSE, warning=FALSE}
model2 <- lm(Sepal.Length ~ Petal.Length + Species, data = iris)

test_performance(model, model2)
```

Although we are satisfied with our model and its performance, let say we are not interested in the effect of `Petal.Length` for different Species, but rather, in its **general trend** "across" all different species. We need to compute the **marginal effect** of the predictor, which corresponds to its slope *averaged* (it's a bit more complex than a simple averaging but the idea is there) over the different factor levels.

```{r message=FALSE, warning=FALSE, eval=FALSE}
slopes <- estimate_slopes(model, trend = "Petal.Length")

slopes
plot(slopes)
```

<!-- # Effects for each factor's levels -->


<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- slopes <- estimate_slopes(model, trend = "Petal.Length", levels = "Species") -->
<!-- # slopes <- estimate_slopes(model, trend = "Petal.Length", modulate = "Petal.Length") -->

<!-- slopes -->
<!-- plot(slopes) -->
<!-- ``` -->


# Interactions between two continuous variables

Also referred to as **Johnson-Neyman intervals**, this plot shows how the effect (the "slope") of one variable varies depending on another variable. It is useful in the case of complex interactions between continuous variables.

For instance, the plot below shows that the effect of `hp` (the y-axis) is significantly negative only when `wt` is low (`< ~4`).

```{r message=FALSE, warning=FALSE}
library(modelbased)

model <- lm(mpg ~ hp * wt, data = mtcars)

slopes <- estimate_slopes(model, trend = "hp", at = "wt")

plot(slopes)
```

# Effect's Derivative

You can also estimate the *derivative* of smooth using `estimate_slopes`.

```{r message=FALSE, warning=FALSE}
# Fit a non-linear General Additive Model (GAM)
model <- mgcv::gam(Sepal.Width ~ s(Petal.Length), data = iris)

# 1. Compute derivative
deriv <- estimate_slopes(model, 
                         trend = "Petal.Length", 
                         at = "Petal.Length",
                         length = 100)

# 2. Visualise
plot(deriv)
```



# References

